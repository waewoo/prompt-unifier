# LiteLLM Configuration for Functional Testing
# Copy this file to .env and fill in your API keys

# =============================================================================
# OpenAI Configuration
# =============================================================================
# For OpenAI GPT models (gpt-4o, gpt-4-turbo, gpt-3.5-turbo)
OPENAI_API_KEY=sk-your-openai-api-key-here

# For GPT-OSS or OpenAI-compatible endpoints
# OPENAI_API_BASE=https://api.openai.com/v1  # Default, uncomment to override
# OPENAI_API_BASE=https://your-gpt-oss-endpoint.com/v1  # Example for GPT-OSS

# =============================================================================
# Anthropic Claude Configuration
# =============================================================================
# For Claude models (claude-3-5-sonnet, claude-3-opus, claude-3-haiku)
# ANTHROPIC_API_KEY=your-anthropic-api-key-here

# =============================================================================
# Mistral Configuration
# =============================================================================
# For Mistral models (mistral-large, codestral, devstral)
# MISTRAL_API_KEY=your-mistral-api-key-here

# =============================================================================
# Local Models (Ollama)
# =============================================================================
# For local Ollama models - no API key required
# Just ensure Ollama is running: http://localhost:11434
# OLLAMA_API_BASE=http://localhost:11434  # Default, uncomment to override

# =============================================================================
# Default Model Configuration
# =============================================================================
# Model to use when 'provider' is not specified in .test.yaml
# Examples:
#   - gpt-4o-mini (OpenAI, cost-effective)
#   - gpt-4o (OpenAI, most capable)
#   - claude-3-5-sonnet-20241022 (Anthropic)
#   - mistral/mistral-large-latest (Mistral)
#   - ollama/devstral (Local Ollama)
DEFAULT_LLM_MODEL=gpt-4o-mini

# =============================================================================
# Advanced Configuration (Optional)
# =============================================================================
# Timeout for AI requests in seconds (default: 60)
# LITELLM_TIMEOUT=120

# Enable debug logging for LiteLLM
# LITELLM_LOG=DEBUG

# =============================================================================
# Other Environment Variables
# =============================================================================
# These are used for other features of prompt-unifier
# SONAR_HOST_URL=http://localhost:9000
# SONAR_TOKEN=your-sonarqube-token

# For PyPI deployment
# PYPI_USER=__token__
# PYPI_PASSWORD=pypi-your-token-here
